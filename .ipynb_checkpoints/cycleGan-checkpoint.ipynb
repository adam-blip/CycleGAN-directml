{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d902e15f-404a-4375-b173-fbdcea671fac",
   "metadata": {},
   "source": [
    "## cyclegan horse2zebra directml\n",
    "\n",
    "- forked from LynnHo/CycleGAN-Tensorflow-2\n",
    "- need windows x64, 8gb shared gpu memory, miniconda \n",
    "- to do: yolov9 arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbd79e6-8168-4f57-8164-fd47533db156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in c:\\users\\user\\miniconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: oyaml in c:\\users\\user\\miniconda3\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow_addons) (24.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\miniconda3\\lib\\site-packages (from oyaml) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons oyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5e2d7-df14-478d-8cfb-8c19092d1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import imlib as im\n",
    "import numpy as np\n",
    "import pylib as py\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tf2lib as tl\n",
    "import tf2gan as gan\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import data\n",
    "import module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1c6b9-7dff-45cf-b3a2-db3dd41c13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                   param                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "args = type('obj', (object,), {\n",
    "    'dataset' : 'horse2zebra',\n",
    "    'datasets_dir' : 'datasets',\n",
    "    'load_size' : 286, # load image to this size\n",
    "    'crop_size' : 256, # then crop to this size\n",
    "    'batch_size' : 1,\n",
    "    'epochs' : 200,\n",
    "    'epoch_decay' : 100, # epoch to start decaying learning rate\n",
    "    'lr' : 0.0002,\n",
    "    'beta_1' : 0.5,\n",
    "    'adversarial_loss_mode' : 'lsgan', #'gan', 'hinge_v1', 'hinge_v2', 'lsgan', 'wgan'\n",
    "    'gradient_penalty_mode' : 'none', #'none', 'dragan', 'wgan-gp'\n",
    "    'gradient_penalty_weight' : 10.0,\n",
    "    'cycle_loss_weight' : 10.0,\n",
    "    'identity_loss_weight' : 0.0,\n",
    "    'pool_size' : 50 # pool size to store fake samples\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04013e4a-0988-43fc-8c95-b6084f90c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                    data                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "A_img_paths = py.glob(py.join(args.datasets_dir, args.dataset, 'trainA'), '*.jpg')\n",
    "B_img_paths = py.glob(py.join(args.datasets_dir, args.dataset, 'trainB'), '*.jpg')\n",
    "A_B_dataset, len_dataset = data.make_zip_dataset(A_img_paths, B_img_paths, args.batch_size, args.load_size, args.crop_size, training=True, repeat=False)\n",
    "\n",
    "A2B_pool = data.ItemPool(args.pool_size)\n",
    "B2A_pool = data.ItemPool(args.pool_size)\n",
    "\n",
    "A_img_paths_test = py.glob(py.join(args.datasets_dir, args.dataset, 'testA'), '*.jpg')\n",
    "B_img_paths_test = py.glob(py.join(args.datasets_dir, args.dataset, 'testB'), '*.jpg')\n",
    "A_B_dataset_test, _ = data.make_zip_dataset(A_img_paths_test, B_img_paths_test, args.batch_size, args.load_size, args.crop_size, training=False, repeat=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8cf5e-e7d2-4178-a57b-54985777ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                   models                                   =\n",
    "# ==============================================================================\n",
    "\n",
    "#G_A2B = module.ResnetGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "#G_B2A = module.ResnetGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "G_A2B = module.yoloGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "G_B2A = module.yoloGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "\n",
    "D_A = module.ConvDiscriminator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "D_B = module.ConvDiscriminator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "\n",
    "d_loss_fn, g_loss_fn = gan.get_adversarial_losses_fn(args.adversarial_loss_mode)\n",
    "cycle_loss_fn = tf.losses.MeanAbsoluteError()\n",
    "identity_loss_fn = tf.losses.MeanAbsoluteError()\n",
    "\n",
    "G_lr_scheduler = module.LinearDecay(args.lr, args.epochs * len_dataset, args.epoch_decay * len_dataset)\n",
    "D_lr_scheduler = module.LinearDecay(args.lr, args.epochs * len_dataset, args.epoch_decay * len_dataset)\n",
    "G_optimizer = keras.optimizers.Adam(learning_rate=G_lr_scheduler, beta_1=args.beta_1)\n",
    "D_optimizer = keras.optimizers.Adam(learning_rate=D_lr_scheduler, beta_1=args.beta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54406d-b20b-4e2f-ab3f-5b8c3559b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                 train step                                 =\n",
    "# ==============================================================================\n",
    "\n",
    "@tf.function\n",
    "def train_G(A, B):\n",
    "    with tf.GradientTape() as t:\n",
    "        A2B = G_A2B(A, training=True)\n",
    "        B2A = G_B2A(B, training=True)\n",
    "        A2B2A = G_B2A(A2B, training=True)\n",
    "        B2A2B = G_A2B(B2A, training=True)\n",
    "        A2A = G_B2A(A, training=True)\n",
    "        B2B = G_A2B(B, training=True)\n",
    "\n",
    "        A2B_d_logits = D_B(A2B, training=True)\n",
    "        B2A_d_logits = D_A(B2A, training=True)\n",
    "\n",
    "        A2B_g_loss = g_loss_fn(A2B_d_logits)\n",
    "        B2A_g_loss = g_loss_fn(B2A_d_logits)\n",
    "        A2B2A_cycle_loss = cycle_loss_fn(A, A2B2A)\n",
    "        B2A2B_cycle_loss = cycle_loss_fn(B, B2A2B)\n",
    "        A2A_id_loss = identity_loss_fn(A, A2A)\n",
    "        B2B_id_loss = identity_loss_fn(B, B2B)\n",
    "\n",
    "        G_loss = (A2B_g_loss + B2A_g_loss) + (A2B2A_cycle_loss + B2A2B_cycle_loss) * args.cycle_loss_weight + (A2A_id_loss + B2B_id_loss) * args.identity_loss_weight\n",
    "\n",
    "    G_grad = t.gradient(G_loss, G_A2B.trainable_variables + G_B2A.trainable_variables)\n",
    "    G_optimizer.apply_gradients(zip(G_grad, G_A2B.trainable_variables + G_B2A.trainable_variables))\n",
    "\n",
    "    return A2B, B2A, {'A2B_g_loss': A2B_g_loss,\n",
    "                      'B2A_g_loss': B2A_g_loss,\n",
    "                      'A2B2A_cycle_loss': A2B2A_cycle_loss,\n",
    "                      'B2A2B_cycle_loss': B2A2B_cycle_loss,\n",
    "                      'A2A_id_loss': A2A_id_loss,\n",
    "                      'B2B_id_loss': B2B_id_loss}\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_D(A, B, A2B, B2A):\n",
    "    with tf.GradientTape() as t:\n",
    "        A_d_logits = D_A(A, training=True)\n",
    "        B2A_d_logits = D_A(B2A, training=True)\n",
    "        B_d_logits = D_B(B, training=True)\n",
    "        A2B_d_logits = D_B(A2B, training=True)\n",
    "\n",
    "        A_d_loss, B2A_d_loss = d_loss_fn(A_d_logits, B2A_d_logits)\n",
    "        B_d_loss, A2B_d_loss = d_loss_fn(B_d_logits, A2B_d_logits)\n",
    "        D_A_gp = gan.gradient_penalty(functools.partial(D_A, training=True), A, B2A, mode=args.gradient_penalty_mode)\n",
    "        D_B_gp = gan.gradient_penalty(functools.partial(D_B, training=True), B, A2B, mode=args.gradient_penalty_mode)\n",
    "\n",
    "        D_loss = (A_d_loss + B2A_d_loss) + (B_d_loss + A2B_d_loss) + (D_A_gp + D_B_gp) * args.gradient_penalty_weight\n",
    "\n",
    "    D_grad = t.gradient(D_loss, D_A.trainable_variables + D_B.trainable_variables)\n",
    "    D_optimizer.apply_gradients(zip(D_grad, D_A.trainable_variables + D_B.trainable_variables))\n",
    "\n",
    "    return {'A_d_loss': A_d_loss + B2A_d_loss,\n",
    "            'B_d_loss': B_d_loss + A2B_d_loss,\n",
    "            'D_A_gp': D_A_gp,\n",
    "            'D_B_gp': D_B_gp}\n",
    "\n",
    "\n",
    "def train_step(A, B):\n",
    "    A2B, B2A, G_loss_dict = train_G(A, B)\n",
    "\n",
    "    # cannot autograph `A2B_pool`\n",
    "    A2B = A2B_pool(A2B)  # or A2B = A2B_pool(A2B.numpy()), but it is much slower\n",
    "    B2A = B2A_pool(B2A)  # because of the communication between CPU and GPU\n",
    "\n",
    "    D_loss_dict = train_D(A, B, A2B, B2A)\n",
    "\n",
    "    return G_loss_dict, D_loss_dict\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample(A, B):\n",
    "    A2B = G_A2B(A, training=False)\n",
    "    B2A = G_B2A(B, training=False)\n",
    "    A2B2A = G_B2A(A2B, training=False)\n",
    "    B2A2B = G_A2B(B2A, training=False)\n",
    "    return A2B, B2A, A2B2A, B2A2B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596bc07-1d3b-43c0-9310-6261d5cfccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                 function for image visualization           =\n",
    "# ==============================================================================\n",
    "\n",
    "def show(image):\n",
    "    image = (image + 1) * .5\n",
    "    imgplot = plt.imshow(image)\n",
    "    plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\n",
    "    plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fa1ab-3ed2-489e-864f-fedfda436366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                    run                                     =\n",
    "# ==============================================================================\n",
    "\n",
    "print('G_A2B:')\n",
    "print(G_A2B.summary())\n",
    "print('G_B2A:')\n",
    "print(G_B2A.summary())\n",
    "print('D_A:')\n",
    "print(D_A.summary())\n",
    "print('D_B:')\n",
    "print(D_B.summary())\n",
    "# epoch counter\n",
    "ep_cnt = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = tl.Checkpoint(dict(G_A2B=G_A2B,\n",
    "                                G_B2A=G_B2A,\n",
    "                                D_A=D_A,\n",
    "                                D_B=D_B,\n",
    "                                G_optimizer=G_optimizer,\n",
    "                                D_optimizer=D_optimizer,\n",
    "                                ep_cnt=ep_cnt),\n",
    "                           py.join(output_dir, 'checkpoints'),\n",
    "                           max_to_keep=5)\n",
    "try:  # restore checkpoint including the epoch counter\n",
    "    checkpoint.restore().assert_existing_objects_matched()\n",
    "    print('checkpoint loaded...')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# summary\n",
    "train_summary_writer = tf.summary.create_file_writer(py.join(output_dir, 'summaries', 'train'))\n",
    "\n",
    "# sample\n",
    "test_iter = iter(A_B_dataset_test)\n",
    "sample_dir = py.join(output_dir, 'samples_training')\n",
    "py.mkdir(sample_dir)\n",
    "\n",
    "\n",
    "# main loop\n",
    "with train_summary_writer.as_default():\n",
    "    for ep in tqdm.trange(args.epochs, desc='Epoch Loop'):\n",
    "        if ep < ep_cnt:\n",
    "            continue\n",
    "\n",
    "        # update epoch counter\n",
    "        ep_cnt.assign_add(1)\n",
    "\n",
    "        # train for an epoch\n",
    "        for A, B in tqdm.tqdm(A_B_dataset, desc='Inner Epoch Loop', total=len_dataset):\n",
    "            G_loss_dict, D_loss_dict = train_step(A, B)\n",
    "\n",
    "            # # summary\n",
    "            tl.summary(G_loss_dict, step=G_optimizer.iterations, name='G_losses')\n",
    "            tl.summary(D_loss_dict, step=G_optimizer.iterations, name='D_losses')\n",
    "            tl.summary({'learning rate': G_lr_scheduler.current_learning_rate}, step=G_optimizer.iterations, name='learning rate')\n",
    "\n",
    "            # sample\n",
    "            # if G_optimizer.iterations.numpy() % 100 == 0:\n",
    "            if G_optimizer.iterations.numpy() % 10 == 0:\n",
    "                clear_output()\n",
    "                A, B = next(test_iter)\n",
    "                A2B, B2A, A2B2A, B2A2B = sample(A, B)\n",
    "                img = im.immerge(np.concatenate([A, A2B, A2B2A, B, B2A, B2A2B], axis=0), n_rows=2)\n",
    "                show(img)\n",
    "                im.imwrite(img, py.join(sample_dir, 'iter-%09d.jpg' % G_optimizer.iterations.numpy()))\n",
    "                \n",
    "\n",
    "        # save checkpoint\n",
    "        checkpoint.save(ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd1c3a-3c8f-42f0-97cf-a01ece76177e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
