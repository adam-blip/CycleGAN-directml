{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d902e15f-404a-4375-b173-fbdcea671fac",
   "metadata": {},
   "source": [
    "## cyclegan horse2zebra directml\n",
    "\n",
    "- forked from LynnHo/CycleGAN-Tensorflow-2\n",
    "- need windows x64, 8gb shared gpu memory, miniconda \n",
    "- to do: yolov9 arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbd79e6-8168-4f57-8164-fd47533db156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-cpu==2.10.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: tensorflow_addons in c:\\users\\user\\miniconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: oyaml in c:\\users\\user\\miniconda3\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.10.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-cpu==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (3.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (24.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (1.17.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\miniconda3\\lib\\site-packages (from oyaml) (6.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2.36.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\miniconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-cpu==2.10.0 tensorflow_addons oyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc5e2d7-df14-478d-8cfb-8c19092d1f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\miniconda3\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "import imlib as im\n",
    "import numpy as np\n",
    "import pylib as py\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kera\n",
    "import tf2lib as tl\n",
    "import tf2gan as gan\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import data\n",
    "import module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f1c6b9-7dff-45cf-b3a2-db3dd41c13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                   param                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "args = type('obj', (object,), {\n",
    "    'dataset' : 'horse2zebra',\n",
    "    'datasets_dir' : 'datasets',\n",
    "    'load_size' : 286, # load image to this size\n",
    "    'crop_size' : 256, # then crop to this size\n",
    "    'batch_size' : 1,\n",
    "    'epochs' : 200,\n",
    "    'epoch_decay' : 100, # epoch to start decaying learning rate\n",
    "    'lr' : 0.0002,\n",
    "    'beta_1' : 0.5,\n",
    "    'adversarial_loss_mode' : 'lsgan', #'gan', 'hinge_v1', 'hinge_v2', 'lsgan', 'wgan'\n",
    "    'gradient_penalty_mode' : 'none', #'none', 'dragan', 'wgan-gp'\n",
    "    'gradient_penalty_weight' : 10.0,\n",
    "    'cycle_loss_weight' : 10.0,\n",
    "    'identity_loss_weight' : 0.0,\n",
    "    'pool_size' : 50 # pool size to store fake samples\n",
    "    \n",
    "})\n",
    "\n",
    "# output_dir\n",
    "output_dir = py.join('output', args.dataset)\n",
    "py.mkdir(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04013e4a-0988-43fc-8c95-b6084f90c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                    data                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "A_img_paths = py.glob(py.join(args.datasets_dir, args.dataset, 'trainA'), '*.jpg')\n",
    "B_img_paths = py.glob(py.join(args.datasets_dir, args.dataset, 'trainB'), '*.jpg')\n",
    "A_B_dataset, len_dataset = data.make_zip_dataset(A_img_paths, B_img_paths, args.batch_size, args.load_size, args.crop_size, training=True, repeat=False)\n",
    "\n",
    "A2B_pool = data.ItemPool(args.pool_size)\n",
    "B2A_pool = data.ItemPool(args.pool_size)\n",
    "\n",
    "A_img_paths_test = py.glob(py.join(args.datasets_dir, args.dataset, 'testA'), '*.jpg')\n",
    "B_img_paths_test = py.glob(py.join(args.datasets_dir, args.dataset, 'testB'), '*.jpg')\n",
    "A_B_dataset_test, _ = data.make_zip_dataset(A_img_paths_test, B_img_paths_test, args.batch_size, args.load_size, args.crop_size, training=False, repeat=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3a0864-5483-4775-8ed5-fcadb6d9d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_normalization(x): # use clipping instead of batchnormalization for network stabilization\n",
    "    return Lambda(lambda x: K.clip(x, -1, 1), output_shape=lambda s: (s[0], s[1], s[2], s[3]))(x)\n",
    "\n",
    "def residual_block(inputs): #combined pixel shuffle and squeeze\n",
    "    x = inputs\n",
    "    x = Conv2D(32, kernel_size = 9, activation = 'tanh', padding = 'same', strides = 2)(x)\n",
    "    x = SeparableConv2D(128, kernel_size = 9, activation = 'tanh', padding = 'same')(x) # rapidly increase speed at slightly worse results\n",
    "    x = fast_normalization(x)\n",
    "    x = Lambda(lambda x: K.reshape(x, (K.shape(x)[0], K.shape(x)[1], K.shape(x)[2], 32, 2, 2)), output_shape = lambda s: (s[0], s[1], s[2], s[3] // 4, 2, 2))(x)\n",
    "    x = Permute((3, 2, 4, 1, 5))(x)\n",
    "    x = Lambda(lambda x: K.reshape(x, (K.shape(x)[0], K.shape(x)[1], K.shape(x)[2] * K.shape(x)[3], K.shape(x)[4] * K.shape(x)[5])), output_shape = lambda s: (s[0], s[1], s[2] * s[3], s[4] * s[5]))(x)\n",
    "    x = Permute((3, 2, 1))(x)\n",
    "    #---\n",
    "    x1 = x\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(8, activation = 'relu')(x) #reduction like in RCAN\n",
    "    x = Dense(32, activation = 'hard_sigmoid')(x)\n",
    "    x = Reshape((1, 1, 32))(x)\n",
    "    x = Multiply()([x1, x])\n",
    "    x = Add()([inputs, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa8cf5e-e7d2-4178-a57b-54985777ecfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m G_lr_scheduler \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mLinearDecay(args\u001b[38;5;241m.\u001b[39mlr, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m*\u001b[39m len_dataset, args\u001b[38;5;241m.\u001b[39mepoch_decay \u001b[38;5;241m*\u001b[39m len_dataset)\n\u001b[0;32m     18\u001b[0m D_lr_scheduler \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mLinearDecay(args\u001b[38;5;241m.\u001b[39mlr, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m*\u001b[39m len_dataset, args\u001b[38;5;241m.\u001b[39mepoch_decay \u001b[38;5;241m*\u001b[39m len_dataset)\n\u001b[1;32m---> 19\u001b[0m G_optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m(learning_rate\u001b[38;5;241m=\u001b[39mG_lr_scheduler, beta_1\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta_1)\n\u001b[0;32m     20\u001b[0m D_optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mD_lr_scheduler, beta_1\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbeta_1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Adam' is not defined"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# =                                   models                                   =\n",
    "# ==============================================================================\n",
    "\n",
    "#G_A2B = module.ResnetGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "#G_B2A = module.ResnetGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "G_A2B = module.yoloGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "G_B2A = module.yoloGenerator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "\n",
    "D_A = module.ConvDiscriminator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "D_B = module.ConvDiscriminator(input_shape=(args.crop_size, args.crop_size, 3))\n",
    "\n",
    "d_loss_fn, g_loss_fn = gan.get_adversarial_losses_fn(args.adversarial_loss_mode)\n",
    "cycle_loss_fn = 'mae'\n",
    "identity_loss_fn = 'mae'\n",
    "\n",
    "G_lr_scheduler = module.LinearDecay(args.lr, args.epochs * len_dataset, args.epoch_decay * len_dataset)\n",
    "D_lr_scheduler = module.LinearDecay(args.lr, args.epochs * len_dataset, args.epoch_decay * len_dataset)\n",
    "G_optimizer = kerasl.opAdam(learning_rate=G_lr_scheduler, beta_1=args.beta_1)\n",
    "D_optimizer = Adam(learning_rate=D_lr_scheduler, beta_1=args.beta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54406d-b20b-4e2f-ab3f-5b8c3559b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                 train step                                 =\n",
    "# ==============================================================================\n",
    "\n",
    "@tf.function\n",
    "def train_G(A, B):\n",
    "    with tf.GradientTape() as t:\n",
    "        A2B = G_A2B(A, training=True)\n",
    "        B2A = G_B2A(B, training=True)\n",
    "        A2B2A = G_B2A(A2B, training=True)\n",
    "        B2A2B = G_A2B(B2A, training=True)\n",
    "        A2A = G_B2A(A, training=True)\n",
    "        B2B = G_A2B(B, training=True)\n",
    "\n",
    "        A2B_d_logits = D_B(A2B, training=True)\n",
    "        B2A_d_logits = D_A(B2A, training=True)\n",
    "\n",
    "        A2B_g_loss = g_loss_fn(A2B_d_logits)\n",
    "        B2A_g_loss = g_loss_fn(B2A_d_logits)\n",
    "        A2B2A_cycle_loss = cycle_loss_fn(A, A2B2A)\n",
    "        B2A2B_cycle_loss = cycle_loss_fn(B, B2A2B)\n",
    "        A2A_id_loss = identity_loss_fn(A, A2A)\n",
    "        B2B_id_loss = identity_loss_fn(B, B2B)\n",
    "\n",
    "        G_loss = (A2B_g_loss + B2A_g_loss) + (A2B2A_cycle_loss + B2A2B_cycle_loss) * args.cycle_loss_weight + (A2A_id_loss + B2B_id_loss) * args.identity_loss_weight\n",
    "\n",
    "    G_grad = t.gradient(G_loss, G_A2B.trainable_variables + G_B2A.trainable_variables)\n",
    "    G_optimizer.apply_gradients(zip(G_grad, G_A2B.trainable_variables + G_B2A.trainable_variables))\n",
    "\n",
    "    return A2B, B2A, {'A2B_g_loss': A2B_g_loss,\n",
    "                      'B2A_g_loss': B2A_g_loss,\n",
    "                      'A2B2A_cycle_loss': A2B2A_cycle_loss,\n",
    "                      'B2A2B_cycle_loss': B2A2B_cycle_loss,\n",
    "                      'A2A_id_loss': A2A_id_loss,\n",
    "                      'B2B_id_loss': B2B_id_loss}\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_D(A, B, A2B, B2A):\n",
    "    with tf.GradientTape() as t:\n",
    "        A_d_logits = D_A(A, training=True)\n",
    "        B2A_d_logits = D_A(B2A, training=True)\n",
    "        B_d_logits = D_B(B, training=True)\n",
    "        A2B_d_logits = D_B(A2B, training=True)\n",
    "\n",
    "        A_d_loss, B2A_d_loss = d_loss_fn(A_d_logits, B2A_d_logits)\n",
    "        B_d_loss, A2B_d_loss = d_loss_fn(B_d_logits, A2B_d_logits)\n",
    "        D_A_gp = gan.gradient_penalty(functools.partial(D_A, training=True), A, B2A, mode=args.gradient_penalty_mode)\n",
    "        D_B_gp = gan.gradient_penalty(functools.partial(D_B, training=True), B, A2B, mode=args.gradient_penalty_mode)\n",
    "\n",
    "        D_loss = (A_d_loss + B2A_d_loss) + (B_d_loss + A2B_d_loss) + (D_A_gp + D_B_gp) * args.gradient_penalty_weight\n",
    "\n",
    "    D_grad = t.gradient(D_loss, D_A.trainable_variables + D_B.trainable_variables)\n",
    "    D_optimizer.apply_gradients(zip(D_grad, D_A.trainable_variables + D_B.trainable_variables))\n",
    "\n",
    "    return {'A_d_loss': A_d_loss + B2A_d_loss,\n",
    "            'B_d_loss': B_d_loss + A2B_d_loss,\n",
    "            'D_A_gp': D_A_gp,\n",
    "            'D_B_gp': D_B_gp}\n",
    "\n",
    "\n",
    "def train_step(A, B):\n",
    "    A2B, B2A, G_loss_dict = train_G(A, B)\n",
    "\n",
    "    # cannot autograph `A2B_pool`\n",
    "    A2B = A2B_pool(A2B)  # or A2B = A2B_pool(A2B.numpy()), but it is much slower\n",
    "    B2A = B2A_pool(B2A)  # because of the communication between CPU and GPU\n",
    "\n",
    "    D_loss_dict = train_D(A, B, A2B, B2A)\n",
    "\n",
    "    return G_loss_dict, D_loss_dict\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample(A, B):\n",
    "    A2B = G_A2B(A, training=False)\n",
    "    B2A = G_B2A(B, training=False)\n",
    "    A2B2A = G_B2A(A2B, training=False)\n",
    "    B2A2B = G_A2B(B2A, training=False)\n",
    "    return A2B, B2A, A2B2A, B2A2B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596bc07-1d3b-43c0-9310-6261d5cfccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                 function for image visualization           =\n",
    "# ==============================================================================\n",
    "\n",
    "def show(image):\n",
    "    image = (image + 1) * .5\n",
    "    imgplot = plt.imshow(image)\n",
    "    plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\n",
    "    plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fa1ab-3ed2-489e-864f-fedfda436366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# =                                    run                                     =\n",
    "# ==============================================================================\n",
    "\n",
    "'''\n",
    "print('G_A2B:')\n",
    "print(G_A2B.summary())\n",
    "print('G_B2A:')\n",
    "print(G_B2A.summary())\n",
    "print('D_A:')\n",
    "print(D_A.summary())\n",
    "print('D_B:')\n",
    "print(D_B.summary())\n",
    "'''\n",
    "# epoch counter\n",
    "ep_cnt = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = tl.Checkpoint(dict(G_A2B=G_A2B,\n",
    "                                G_B2A=G_B2A,\n",
    "                                D_A=D_A,\n",
    "                                D_B=D_B,\n",
    "                                G_optimizer=G_optimizer,\n",
    "                                D_optimizer=D_optimizer,\n",
    "                                ep_cnt=ep_cnt),\n",
    "                           py.join(output_dir, 'checkpoints'),\n",
    "                           max_to_keep=5)\n",
    "try:  # restore checkpoint including the epoch counter\n",
    "    checkpoint.restore().assert_existing_objects_matched()\n",
    "    print('checkpoint loaded...')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# summary\n",
    "train_summary_writer = tf.summary.create_file_writer(py.join(output_dir, 'summaries', 'train'))\n",
    "\n",
    "# sample\n",
    "test_iter = iter(A_B_dataset_test)\n",
    "sample_dir = py.join(output_dir, 'samples_training')\n",
    "py.mkdir(sample_dir)\n",
    "\n",
    "\n",
    "# main loop\n",
    "with train_summary_writer.as_default():\n",
    "    for ep in tqdm.trange(args.epochs, desc='Epoch Loop'):\n",
    "        if ep < ep_cnt:\n",
    "            continue\n",
    "\n",
    "        # update epoch counter\n",
    "        ep_cnt.assign_add(1)\n",
    "\n",
    "        # train for an epoch\n",
    "        for A, B in tqdm.tqdm(A_B_dataset, desc='Inner Epoch Loop', total=len_dataset):\n",
    "            G_loss_dict, D_loss_dict = train_step(A, B)\n",
    "\n",
    "            # # summary\n",
    "            tl.summary(G_loss_dict, step=G_optimizer.iterations, name='G_losses')\n",
    "            tl.summary(D_loss_dict, step=G_optimizer.iterations, name='D_losses')\n",
    "            tl.summary({'learning rate': G_lr_scheduler.current_learning_rate}, step=G_optimizer.iterations, name='learning rate')\n",
    "\n",
    "            # sample\n",
    "            # if G_optimizer.iterations.numpy() % 100 == 0:\n",
    "            if G_optimizer.iterations.numpy() % 10 == 0:\n",
    "                clear_output()\n",
    "                A, B = next(test_iter)\n",
    "                A2B, B2A, A2B2A, B2A2B = sample(A, B)\n",
    "                img = im.immerge(np.concatenate([A, A2B, A2B2A, B, B2A, B2A2B], axis=0), n_rows=2)\n",
    "                show(img)\n",
    "                im.imwrite(img, py.join(sample_dir, 'iter-%09d.jpg' % G_optimizer.iterations.numpy()))\n",
    "                \n",
    "\n",
    "        # save checkpoint\n",
    "        checkpoint.save(ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd1c3a-3c8f-42f0-97cf-a01ece76177e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
